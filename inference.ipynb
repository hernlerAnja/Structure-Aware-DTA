{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f70ae20",
   "metadata": {},
   "source": [
    "# Inference - run pretrained model with kiba data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679236d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set which GPU to use (here, GPU 2) by setting the CUDA_VISIBLE_DEVICES environment variable \n",
    "# (important if executed on server, check which GPU is available)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237c0630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpu name: Tesla P40\n",
      "Memory allocated: 0.0 GB\n",
      "Memory cached: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_id = device.index\n",
    "print('Gpu name:', torch.cuda.get_device_name(gpu_id))\n",
    "print('Memory allocated:', round(torch.cuda.memory_allocated(gpu_id) / (1024 ** 3),2), 'GB')\n",
    "print('Memory cached:', round(torch.cuda.memory_reserved(gpu_id) / (1024 ** 3),2), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9521e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the tankbind source folder to the Python path (needed for imports from tankbind to work)\n",
    "tankbind_src_folder_path = \"./tankbind/\"\n",
    "sys.path.insert(0, tankbind_src_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bffcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tankbind\n",
    "from feature_utils import get_protein_feature, get_clean_res_list, extract_torchdrug_feature_from_mol, get_canonical_smiles\n",
    "from utils import construct_data_from_graph_gvp, evaulate_with_affinity, evaulate\n",
    "from model import get_model\n",
    "from generation_utils import get_LAS_distance_constraint_mask, get_info_pred_distance, write_with_new_coords\n",
    "from metrics import print_metrics, myMetric\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from Bio.PDB import PDBParser\n",
    "import torchmetrics\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\") # NOTE: only uncomment if appearing warnings are not relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625ac51",
   "metadata": {},
   "source": [
    "## Load molecule_dict and protein_dict & kiba_data pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load protein and molecule dictionaries & filtered kiba_data\n",
    "protein_dict = torch.load(\"data/protein_dict.pt\")\n",
    "molecule_dict = torch.load(\"data/molecule_dict.pt\")\n",
    "kiba_data = torch.load('data/kiba_data_small_dismap.pt') # NOTE: kiba_data is the complete DataFrame with the P2Rank information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a40ff5",
   "metadata": {},
   "source": [
    "### Remove samples with dis_map > 10000 (due to memory issues) --> removed 12620 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f37dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118254it [18:46, 104.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements with dis_map size > 10000: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment & execute only if the dataset was previously created with the unfiltered kiba dataset (inlcuding dis_map size > 10000)\n",
    "\n",
    "# # check & drop dis_map size > 10000\n",
    "# data_loader = DataLoader(dataset, batch_size=1, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0)\n",
    "# indices = [] # list to store indices of elements with dis_map size > 10000\n",
    "# for i, (elem, y) in tqdm(enumerate(data_loader)):\n",
    "#    if elem.dis_map.shape[0] > 10000:\n",
    "#       indices.append(i)\n",
    "# print(f\"Number of elements with dis_map size > 10000: {len(indices)}\")\n",
    "\n",
    "# data_no_dis_map = dataset.data\n",
    "# data_no_dis_map = data_no_dis_map.drop(indices)  # drop the elements with dis_map size > 10000\n",
    "# torch.save(data_no_dis_map, 'data/kiba_data_small_dismap.pt')  # save the modified dataset without dis_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280ba22",
   "metadata": {},
   "source": [
    "# Dataset class & Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0362e",
   "metadata": {},
   "source": [
    "I also return the target affinities together with the model input since some of the inputs might be discarded during training due to memory size issues. So I return both to keep them correctly assigned/ordered.\n",
    "\n",
    "NOTE: The dataset class is copied from the Tankbind repository, with only some small changes to also return the target affinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46138be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset_VS(Dataset):\n",
    "    def __init__(self, root, data=None, protein_dict=None, molecule_dict=None, proteinMode=0, compoundMode=1,\n",
    "                 pocket_radius=20, shake_nodes=None,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data = data\n",
    "        self.protein_dict = protein_dict\n",
    "        self.molecule_dict = molecule_dict\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data = torch.load(self.processed_paths[0])\n",
    "        self.protein_dict = torch.load(self.processed_paths[1])\n",
    "        self.molecule_dict = torch.load(self.processed_paths[2])\n",
    "        self.proteinMode = proteinMode\n",
    "        self.pocket_radius = pocket_radius\n",
    "        self.compoundMode = compoundMode\n",
    "        self.shake_nodes = shake_nodes\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['kiba_data.pt', 'protein_dict.pt', 'molecule_dict.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Save data and protein dictionary\n",
    "        torch.save(self.data, self.processed_paths[0])\n",
    "        torch.save(self.protein_dict, self.processed_paths[1])\n",
    "        torch.save(self.molecule_dict, self.processed_paths[2])\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        smiles = line['smiles']\n",
    "        target_affinity = line['target_affinity'] # get the target affinity value so we can return it together with the data (used for evaluation)\n",
    "        pocket_com = line['pocket_com']\n",
    "        pocket_com = np.array(pocket_com.split(\",\")).astype(float) if isinstance(pocket_com, str) else pocket_com\n",
    "        pocket_com = pocket_com.reshape((1, 3))\n",
    "        use_whole_protein = line.get('use_whole_protein', False)\n",
    "\n",
    "        protein_name = line['protein_name']\n",
    "        protein_data = self.protein_dict.get(protein_name)\n",
    "        \n",
    "        if protein_data is None:\n",
    "            raise ValueError(f\"Protein {protein_name} not found in pre-calculated protein dictionary\")\n",
    "\n",
    "        protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = protein_data\n",
    "\n",
    "        # Load precomputed molecular features\n",
    "        molecule_data = self.molecule_dict.get(smiles)\n",
    "        if molecule_data is None:\n",
    "            raise ValueError(f\"SMILES {smiles} not found in precomputed molecular dictionary\")\n",
    "        \n",
    "        coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = self.molecule_dict[smiles]\n",
    "\n",
    "        data, input_node_list, keepNode = construct_data_from_graph_gvp(\n",
    "            protein_node_xyz, protein_seq, protein_node_s, protein_node_v, \n",
    "            protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "            coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "            pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "            use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode\n",
    "        )\n",
    "        data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "        \n",
    "        return data, target_affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122855a4",
   "metadata": {},
   "source": [
    "### Create dataset instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b754863",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data' # Specify the path where the dataset will be stored\n",
    "\n",
    "# dataset = MyDataset_VS(root=dataset_path, data=kiba_data, protein_dict=protein_dict, molecule_dict=molecule_dict) # NOTE: use this only on first run, otherwise execute line below\n",
    "\n",
    "dataset = MyDataset_VS(root=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3f84e",
   "metadata": {},
   "source": [
    "# Model testing - takes around 33h on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/system/user/studentwork/hernler/./tankbind/model.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model\n",
    "import importlib\n",
    "\n",
    "# reload the whole module so changes in IaBNet_with_affinity and get_model are reflected\n",
    "importlib.reload(model)\n",
    "\n",
    "# NOTE: forced reload is needed so the changes made in the Tankbind files are actually applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba7c89",
   "metadata": {},
   "source": [
    "### Masking function for bringing the vector representations to the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a226800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert z to same size vector representation \n",
    "def masked_mean_pool(z, z_mask):\n",
    "    z_mask_unsqueezed = z_mask.unsqueeze(-1)  # [B, P, C, 1]\n",
    "    masked_z = z * z_mask_unsqueezed\n",
    "    sum_z = masked_z.sum(dim=(1, 2))  # [B, H]\n",
    "    norm = z_mask_unsqueezed.sum(dim=(1, 2)) + 1e-6  # [B, 1]\n",
    "    return sum_z / norm  # [B, H]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears GPU memory - only uncomment/execute if necessary\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()  # Optional: cleans up inter-process memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:00:16   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3213074dd843bebcc30374cde420d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !!!! Code without dis_map check: !!!! NOTE: use this only with the filtered dataset\n",
    "\n",
    "batch_size = 6 # NOTE: max batchsize for for GPU with 24GB RAM (with higher batchsize --> I recieved a memory error when testing was around 97% complete)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = get_model(0, logging, device)\n",
    "\n",
    "# load pretrainded self-dock model\n",
    "modelFile = \"model/self_dock.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "_ = model.eval()\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0)\n",
    "\n",
    "affinity_pred_list = []\n",
    "vector_representations = []\n",
    "\n",
    "for x, y in tqdm(data_loader):     \n",
    "    x = x.to(device) # only move x to device as y is not used in the model\n",
    "    y_pred, affinity_pred = model(x)\n",
    "\n",
    "    vector_repr = masked_mean_pool(model.vec_repr, model.z_mask) # apply the masked mean pooling to the vector representation\n",
    "\n",
    "    affinity_pred_list.append(affinity_pred.detach().cpu())\n",
    "    vector_representations.append(vector_repr.detach().cpu())\n",
    "\n",
    "# concatenate the lists into tensors\n",
    "affinity_pred_list = torch.cat(affinity_pred_list)\n",
    "vector_representations = torch.cat(vector_representations)\n",
    "\n",
    "# save the affinity predictions & vector representations\n",
    "torch.save(affinity_pred_list, 'data/affinity_pred.pt')\n",
    "torch.save(vector_representations, 'vector_representations/vector_representations.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affinity_pred_list shape: torch.Size([105634])\n",
      "vector_representations shape: torch.Size([105634, 128])\n",
      "vector_representations[0] shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# check tensor shapes\n",
    "print(f\"affinity_pred_list shape: {affinity_pred_list.shape}\")\n",
    "print(f\"vector_representations shape: {vector_representations.shape}\")\n",
    "print(f\"vector_representations[0] shape: {vector_representations[0].shape}\") # shape of the first vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e728afb",
   "metadata": {},
   "source": [
    "### Add affinity predictions to kiba dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a473fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>compound_name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pocket_name</th>\n",
       "      <th>pocket_com</th>\n",
       "      <th>target_affinity</th>\n",
       "      <th>affinity_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2R5T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl</td>\n",
       "      <td>best_p2rank_pocket</td>\n",
       "      <td>32.53,34.506,67.174</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6.020516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3BRT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl</td>\n",
       "      <td>best_p2rank_pocket</td>\n",
       "      <td>14.396,20.696,11.566</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1.521520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3BRT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl</td>\n",
       "      <td>best_p2rank_pocket</td>\n",
       "      <td>14.396,20.696,11.566</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1.521520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1IVO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl</td>\n",
       "      <td>best_p2rank_pocket</td>\n",
       "      <td>115.598,69.377,45.458</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2.156317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1MFG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl</td>\n",
       "      <td>best_p2rank_pocket</td>\n",
       "      <td>8.068,0.821,17.188</td>\n",
       "      <td>11.1</td>\n",
       "      <td>3.940505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 protein_name  compound_name  \\\n",
       "0           0         2R5T            NaN   \n",
       "1           1         3BRT            NaN   \n",
       "2           2         3BRT            NaN   \n",
       "3           3         1IVO            NaN   \n",
       "4           4         1MFG            NaN   \n",
       "\n",
       "                                          smiles         pocket_name  \\\n",
       "0  COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl  best_p2rank_pocket   \n",
       "1  COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl  best_p2rank_pocket   \n",
       "2  COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl  best_p2rank_pocket   \n",
       "3  COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl  best_p2rank_pocket   \n",
       "4  COC1=C(C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl  best_p2rank_pocket   \n",
       "\n",
       "              pocket_com  target_affinity  affinity_pred  \n",
       "0    32.53,34.506,67.174             11.1       6.020516  \n",
       "1   14.396,20.696,11.566             11.1       1.521520  \n",
       "2   14.396,20.696,11.566             11.1       1.521520  \n",
       "3  115.598,69.377,45.458             11.1       2.156317  \n",
       "4     8.068,0.821,17.188             11.1       3.940505  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kiba_df = dataset.data\n",
    "\n",
    "# if new session, load the affinity predictions & the kiba df\n",
    "affinity_pred_list = torch.load('data/affinity_pred.pt')\n",
    "kiba_df = torch.load('data/kiba_data_small_dismap.pt')  # load the filtered kiba_df (used for inference)\n",
    "\n",
    "kiba_df['affinity_pred'] = affinity_pred_list\n",
    "kiba_df.head()\n",
    "# save the updated kiba_df with affinity predictions\n",
    "# kiba_df.to_csv('data/kiba_data_with_affinity_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c31b0",
   "metadata": {},
   "source": [
    "# Evaluation of the predicted affinities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcda33c",
   "metadata": {},
   "source": [
    "**Mean squared error, mean absolute error & r2-score:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(targets, predictions):\n",
    "    mse = torchmetrics.functional.mean_squared_error(torch.tensor(predictions), torch.tensor(targets))\n",
    "    mae = torchmetrics.functional.mean_absolute_error(torch.tensor(predictions), torch.tensor(targets))\n",
    "    r2 = torchmetrics.functional.r2_score(torch.tensor(predictions), torch.tensor(targets))\n",
    "    return mse.item(), mae.item(), r2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314cfe2",
   "metadata": {},
   "source": [
    "**Concordance index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215374e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_idx(y_true, y_pred):\n",
    "    # 1. Convert inputs to NumPy arrays\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # 2. Sort by predicted values\n",
    "    order = np.argsort(y_pred)\n",
    "    y_true = y_true[order]\n",
    "\n",
    "    # 3. Find unique labels and group sizes\n",
    "    unique_labels, inverse = np.unique(y_true, return_inverse=True)\n",
    "    counts = np.bincount(inverse)  # number of samples per unique label\n",
    "\n",
    "    print(f\"Found {len(unique_labels)} unique labels\")\n",
    "\n",
    "    # 4. Compute cumulative counts\n",
    "    cum_counts = np.cumsum(counts)  # prefix sum for fast pair counting\n",
    "\n",
    "    concordant = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    # 5. Iterate over label groups\n",
    "    for i, count_i in enumerate(tqdm(counts, desc=\"Processing label groups\")):\n",
    "        # Total pairs with higher labels (since sorted by prediction, no pair expansion)\n",
    "        higher = cum_counts[-1] - cum_counts[i]\n",
    "        total_pairs += count_i * higher\n",
    "\n",
    "        # Since it is sorted by prediction, all these pairs are concordant\n",
    "        concordant += count_i * higher\n",
    "\n",
    "    c_index = concordant / total_pairs if total_pairs > 0 else np.nan\n",
    "\n",
    "    return c_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1846bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae, r2 = eval_metrics(kiba_df['target_affinity'].values, affinity_pred_list)\n",
    "c_index = concordance_idx(kiba_df['target_affinity'].values, affinity_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b058f",
   "metadata": {},
   "source": [
    "OR (old function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3698b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(preds, targets):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        mse = criterion(preds, targets)\n",
    "        mae = torch.mean(torch.abs(preds - targets))\n",
    "    return mse.item(), mae.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da1c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 36.7511\n",
      "Mean Absolute Error: 5.6623\n"
     ]
    }
   ],
   "source": [
    "preds = torch.tensor(kiba_df['affinity_pred'].to_list(), requires_grad=True, device=device)\n",
    "targets = torch.tensor(kiba_df['target_affinity'].to_list(), device=device)\n",
    "\n",
    "mse, mae = eval_metrics(preds, targets)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289aeaca",
   "metadata": {},
   "source": [
    "**Concordance index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98777d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_idx(y_true, y_pred):\n",
    "    print(\"[Step 1] Converting inputs to NumPy arrays...\")\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    print(\"[Step 2] Sorting by predicted values...\")\n",
    "    order = np.argsort(y_pred)\n",
    "    y_true = y_true[order]\n",
    "\n",
    "    print(\"[Step 3] Finding unique labels and group sizes...\")\n",
    "    unique_labels, inverse = np.unique(y_true, return_inverse=True)\n",
    "    counts = np.bincount(inverse)  # number of samples per unique label\n",
    "\n",
    "    print(f\"    Found {len(unique_labels)} unique labels\")\n",
    "\n",
    "    print(\"[Step 4] Computing cumulative counts...\")\n",
    "    cum_counts = np.cumsum(counts)  # prefix sum for fast pair counting\n",
    "\n",
    "    concordant = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    print(\"[Step 5] Iterating over label groups...\")\n",
    "    for i, count_i in enumerate(tqdm(counts, desc=\"Processing label groups\")):\n",
    "        # Total pairs with higher labels (since sorted by prediction, no pair expansion)\n",
    "        higher = cum_counts[-1] - cum_counts[i]\n",
    "        total_pairs += count_i * higher\n",
    "\n",
    "        # Since we sorted by prediction, all these pairs are concordant\n",
    "        concordant += count_i * higher\n",
    "\n",
    "    print(\"[Step 6] Finalizing concordance index...\")\n",
    "    c_index = concordant / total_pairs if total_pairs > 0 else np.nan\n",
    "    print(f\"[Done] Concordance Index: {c_index:.4f}\")\n",
    "\n",
    "    return c_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52a42ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Converting inputs to NumPy arrays...\n",
      "[Step 2] Sorting by predicted values...\n",
      "[Step 3] Finding unique labels and group sizes...\n",
      "    Found 2678 unique labels\n",
      "[Step 4] Computing cumulative counts...\n",
      "[Step 5] Iterating over label groups...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce9ebd835f7468f8694557ad90ee0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing label groups:   0%|          | 0/2678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6] Finalizing concordance index...\n",
      "[Done] Concordance Index: 1.0000\n"
     ]
    }
   ],
   "source": [
    "c_index = concordance_idx(kiba_df['target_affinity'].to_numpy(), kiba_df['affinity_pred'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077c0f3",
   "metadata": {},
   "source": [
    "## Save target affinities for model training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bea6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(kiba_df['target_affinity'].values)\n",
    "\n",
    "torch.save(targets, \"vector_representations/labels.pt\")\n",
    "\n",
    "# load with:\n",
    "# labels = torch.load(\"vector_representations/labels.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
