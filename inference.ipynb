{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f70ae20",
   "metadata": {},
   "source": [
    "# Inference - run pretrained model with kiba data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9521e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "tankbind_src_folder_path = \"./tankbind/\"\n",
    "sys.path.insert(0, tankbind_src_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bffcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tankbind\n",
    "from feature_utils import get_protein_feature, get_clean_res_list, extract_torchdrug_feature_from_mol, get_canonical_smiles\n",
    "from utils import construct_data_from_graph_gvp, evaulate_with_affinity, evaulate\n",
    "from model import get_model\n",
    "from generation_utils import get_LAS_distance_constraint_mask, get_info_pred_distance, write_with_new_coords\n",
    "from metrics import print_metrics, myMetric\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from Bio.PDB import PDBParser\n",
    "import torchmetrics\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\") # NOTE: only uncomment if appearing warnings are not relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625ac51",
   "metadata": {},
   "source": [
    "## Load molecule_dict and protein_dict & kiba_data pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bd224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load protein and molecule dictionaries & kiba_data\n",
    "protein_dict = torch.load(\"data/protein_dict.pt\")\n",
    "molecule_dict = torch.load(\"data/molecule_dict.pt\")\n",
    "kiba_data = torch.load('data/kiba_data.pt') # NOTE: kiba_data is the complete DataFrame with the P2Rank information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280ba22",
   "metadata": {},
   "source": [
    "# Dataset class + Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0362e",
   "metadata": {},
   "source": [
    "I also return the target affinities together with the model input since some of the inputs might be discarded during training due to memory size issues. So I return both to keep them correctly assigned/ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46138be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset_VS(Dataset):\n",
    "    def __init__(self, root, data=None, protein_dict=None, molecule_dict=None, proteinMode=0, compoundMode=1,\n",
    "                 pocket_radius=20, shake_nodes=None,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data = data\n",
    "        self.protein_dict = protein_dict\n",
    "        self.molecule_dict = molecule_dict\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data = torch.load(self.processed_paths[0])\n",
    "        self.protein_dict = torch.load(self.processed_paths[1])\n",
    "        self.molecule_dict = torch.load(self.processed_paths[2])\n",
    "        self.proteinMode = proteinMode\n",
    "        self.pocket_radius = pocket_radius\n",
    "        self.compoundMode = compoundMode\n",
    "        self.shake_nodes = shake_nodes\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['kiba_data.pt', 'protein_dict.pt', 'molecule_dict.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Save data and protein dictionary\n",
    "        torch.save(self.data, self.processed_paths[0])\n",
    "        torch.save(self.protein_dict, self.processed_paths[1])\n",
    "        torch.save(self.molecule_dict, self.processed_paths[2])\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        smiles = line['smiles']\n",
    "        target_affinity = line['target_affinity'] # get the target affinity value so we can return it together with the data (used for evaluation) - TODO: is it necessary, sine we have our dataframe anyway and we don't shuffle the dataset?\n",
    "        pocket_com = line['pocket_com']\n",
    "        pocket_com = np.array(pocket_com.split(\",\")).astype(float) if isinstance(pocket_com, str) else pocket_com\n",
    "        pocket_com = pocket_com.reshape((1, 3))\n",
    "        use_whole_protein = line.get('use_whole_protein', False)\n",
    "\n",
    "        protein_name = line['protein_name']\n",
    "        protein_data = self.protein_dict.get(protein_name)\n",
    "        \n",
    "        if protein_data is None:\n",
    "            raise ValueError(f\"Protein {protein_name} not found in pre-calculated protein dictionary\")\n",
    "\n",
    "        protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = protein_data\n",
    "\n",
    "        # Load precomputed molecular features\n",
    "        molecule_data = self.molecule_dict.get(smiles)\n",
    "        if molecule_data is None:\n",
    "            raise ValueError(f\"SMILES {smiles} not found in precomputed molecular dictionary\")\n",
    "        \n",
    "        coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = self.molecule_dict[smiles]\n",
    "\n",
    "        data, input_node_list, keepNode = construct_data_from_graph_gvp(\n",
    "            protein_node_xyz, protein_seq, protein_node_s, protein_node_v, \n",
    "            protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "            coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "            pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "            use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode\n",
    "        )\n",
    "        data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "        \n",
    "        return data, target_affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122855a4",
   "metadata": {},
   "source": [
    "### Create dataset instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b754863",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data' # Specify the path where the dataset will be stored\n",
    "# dataset = MyDataset_VS(root=dataset_path, data=kiba_data, protein_dict=protein_dict, molecule_dict=molecule_dict) # NOTE: use this only on first run, otherwise execute line below\n",
    "dataset = MyDataset_VS(root=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0e76f",
   "metadata": {},
   "source": [
    "### Remove samples with dis_map > 10000 (memory issues) --> removed 12620 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f37dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118254it [18:46, 104.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements with dis_map size > 10000: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check dismap size:\n",
    "data_loader = DataLoader(dataset, batch_size=1, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0)\n",
    "\n",
    "indices = [] # list to store indices of elements with dis_map size > 10000\n",
    "for i, (elem, y) in tqdm(enumerate(data_loader)):\n",
    "   if elem.dis_map.shape[0] > 10000:\n",
    "      indices.append(i)\n",
    "print(f\"Number of elements with dis_map size > 10000: {len(indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdff1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_dis_map = dataset.data\n",
    "data_no_dis_map = data_no_dis_map.drop(indices)  # drop the elements with dis_map size > 10000\n",
    "torch.save(data_no_dis_map, 'data/kiba_data_small_dismap.pt')  # save the modified dataset without dis_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8571bb9",
   "metadata": {},
   "source": [
    "### Create filtered dataset instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data/filtered' # Specify the path where the dataset will be stored\n",
    "# dataset = MyDataset_VS(root=dataset_path, data=data_no_dis_map, protein_dict=protein_dict, molecule_dict=molecule_dict) # NOTE: use this only on first run, otherwise execute line below\n",
    "dataset = MyDataset_VS(root=dataset_path)# smaller dataset without dis_map>10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3f84e",
   "metadata": {},
   "source": [
    "# Model testing - takes around 33h on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "605cbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "140e893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'c:\\\\Users\\\\anja\\\\Documents\\\\GitHub\\\\Bachelor-Thesis-AI\\\\./tankbind\\\\model.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model\n",
    "import importlib\n",
    "\n",
    "# reload the whole module so changes in IaBNet_with_affinity and get_model are reflected\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba7c89",
   "metadata": {},
   "source": [
    "### Masking function for bringing the vector representations to the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a226800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert z to same size vector representation \n",
    "def masked_mean_pool(z, z_mask):\n",
    "    z_mask_unsqueezed = z_mask.unsqueeze(-1)  # [B, P, C, 1]\n",
    "    masked_z = z * z_mask_unsqueezed\n",
    "    sum_z = masked_z.sum(dim=(1, 2))  # [B, H]\n",
    "    norm = z_mask_unsqueezed.sum(dim=(1, 2)) + 1e-6  # [B, 1]\n",
    "    return sum_z / norm  # [B, H]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:32:25   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/26409 [00:08<59:21:51,  8.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# !!!! Code without dis_map check: !!!! NOTE: use this only with the filtered dataset\n",
    "\n",
    "batch_size = 4 # NOTE: max batchsize for local cpu execution\n",
    "i = 0\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = get_model(0, logging, device)\n",
    "\n",
    "# pretrainded self-dock model\n",
    "modelFile = \"model/self_dock.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "_ = model.eval()\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0)\n",
    "affinity_pred_list = []\n",
    "y_pred_list = [] # add code to save y_preds\n",
    "vector_representations = []\n",
    "for x, y in tqdm(data_loader):        \n",
    "    x = x.to(device) # only move x to device as y is not used in the model\n",
    "    y_pred, affinity_pred = model(x)\n",
    "\n",
    "    vector_repr = masked_mean_pool(model.vec_repr, model.z_mask) # apply the masked mean pooling to the vector representation\n",
    "\n",
    "    affinity_pred_list.append(affinity_pred.detach().cpu())\n",
    "    vector_representations.append(vector_repr.detach().cpu())\n",
    "    \n",
    "    i+=1\n",
    "    if i >= 2: break # remove this break to run over the whole dataset\n",
    "\n",
    "\n",
    "affinity_pred_list = torch.cat(affinity_pred_list)\n",
    "vector_representations = torch.cat(vector_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a6ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:29:39   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 104/118254 [01:02<19:43:08,  1.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdis_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10000\u001b[39m: \u001b[38;5;66;03m# remove large proteins (due to memory issues) - NOTE: adjust value to batch size and GPU memory (e.g. 100000 for batch_size=10)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# only move x to device as y is not used in the model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     y_pred, affinity_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     vector_repr \u001b[38;5;241m=\u001b[39m masked_mean_pool(model\u001b[38;5;241m.\u001b[39mvec_repr, model\u001b[38;5;241m.\u001b[39mz_mask) \u001b[38;5;66;03m# apply the masked mean pooling to the vector representation\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     affinity_pred_list\u001b[38;5;241m.\u001b[39mappend(affinity_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[1;32mc:\\Users\\anja\\miniconda3\\envs\\rdkit-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\anja\\Documents\\GitHub\\Bachelor-Thesis-AI\\./tankbind\\model.py:385\u001b[0m, in \u001b[0;36mIaBNet_with_affinity.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    383\u001b[0m             z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    384\u001b[0m             z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriangle_self_attention_list[i_module](z, z_mask))\n\u001b[1;32m--> 385\u001b[0m             z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranistion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# batch_dim = z.shape[0]\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m#################################### changes to the model ########################################\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvec_repr \u001b[38;5;241m=\u001b[39m z\n",
      "File \u001b[1;32mc:\\Users\\anja\\miniconda3\\envs\\rdkit-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\anja\\Documents\\GitHub\\Bachelor-Thesis-AI\\./tankbind\\model.py:272\u001b[0m, in \u001b[0;36mTransition.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(z))\u001b[38;5;241m.\u001b[39mrelu())\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[1;32mc:\\Users\\anja\\miniconda3\\envs\\rdkit-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\anja\\miniconda3\\envs\\rdkit-env\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anja\\miniconda3\\envs\\rdkit-env\\lib\\site-packages\\torch\\nn\\functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2501\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2502\u001b[0m     )\n\u001b[1;32m-> 2503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 4 # NOTE: max batchsize for local cpu execution\n",
    "i = 0\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = get_model(0, logging, device)\n",
    "\n",
    "# pretrainded self-dock model\n",
    "modelFile = \"model/self_dock.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "_ = model.eval()\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0)\n",
    "affinity_pred_list = []\n",
    "y_pred_list = [] # add code to save y_preds\n",
    "vector_representations = []\n",
    "for x, y in tqdm(data_loader):        \n",
    "    if x.dis_map.shape[0] < 40000: # remove large proteins (due to memory issues) - NOTE: adjust value to batch size and GPU memory (e.g. 100000 for batch_size=10)\n",
    "        x = x.to(device) # only move x to device as y is not used in the model\n",
    "        y_pred, affinity_pred = model(x)\n",
    "\n",
    "        vector_repr = masked_mean_pool(model.vec_repr, model.z_mask) # apply the masked mean pooling to the vector representation\n",
    "\n",
    "        affinity_pred_list.append(affinity_pred.detach().cpu())\n",
    "        vector_representations.append(vector_repr.detach().cpu())\n",
    "    else:\n",
    "        affinity_pred_list.append(torch.zeros(batch_size).detach().cpu())\n",
    "        vector_representations.append(torch.zeros((batch_size, 128)).detach().cpu()) # TODO: check if 128 is the correct size for vector_repr --> should be fine (masking function reurns (batch_size, 128) vector)\n",
    "    i+=1\n",
    "    if i >= 2: break # remove this break to run over the whole dataset\n",
    "\n",
    "\n",
    "affinity_pred_list = torch.cat(affinity_pred_list)\n",
    "vector_representations = torch.cat(vector_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c96b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affinity_pred_list shape: torch.Size([8])\n",
      "vector_representations shape: torch.Size([8, 128])\n",
      "vector_representations[0] shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"affinity_pred_list shape: {affinity_pred_list.shape}\")\n",
    "print(f\"vector_representations shape: {vector_representations.shape}\")\n",
    "print(f\"vector_representations[0] shape: {vector_representations[0].shape}\") # print the shape of the first vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the affinity predictions\n",
    "torch.save(affinity_pred_list, 'data/affinity_pred.pt')\n",
    "\n",
    "# save the vector representations\n",
    "torch.save(vector_representations, 'vector_representations/vector_representations.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e728afb",
   "metadata": {},
   "source": [
    "### Add affinity predictions to kiba dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a473fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiba_df = dataset.data\n",
    "kiba_df['affinity_pred'] = affinity_pred_list\n",
    "\n",
    "# save the updated kiba_df with affinity predictions\n",
    "kiba_df.to_csv('data/kiba_data_with_affinity_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7ac3c",
   "metadata": {},
   "source": [
    "### Add vector representations to kiba df (if needed/possible), --> not at the moment: since labels [target_affinity] are already linked with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cded5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if this is needed, works like this\n",
    "# Note: currently the vector_reprs are a list with tuples of (vector_repr, target_affinity)\n",
    "\n",
    "kiba_df['vector_repr'] = vector_representations.tolist()  # Convert tensor to list for DataFrame compatibility\n",
    "# save the updated kiba_df with vector representations \n",
    "kiba_df.to_csv('vector_representations/kiba_data_with_vector_repr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c31b0",
   "metadata": {},
   "source": [
    "# Evaluation of the predicted affinities (needed for comparison with new model) - TODO: add concordance index metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "mse = torchmetrics.functional.mean_squared_error(kiba_df[\"affinity_pred\"].values, kiba_df['target_affinity'].values, squared=False)\n",
    "\n",
    "# mean absolute error\n",
    "mae = torchmetrics.functional.mean_absolute_error(kiba_df['affinity_pred'].values, kiba_df['target_affinity'].values)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse.item()}\")\n",
    "print(f\"Mean Absolute Error: {mae.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b058f",
   "metadata": {},
   "source": [
    "OR (old function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(preds, targets):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        mse = criterion(preds, targets)\n",
    "        mae = torch.mean(torch.abs(preds - targets))\n",
    "    return mse.item(), mae.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor(kiba_df['affinity_pred'].to_list(), requires_grad=True, device=device)\n",
    "targets = torch.tensor(kiba_df['target_affinity'].to_list())\n",
    "\n",
    "mse, mae = eval_metrics(preds, targets)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
